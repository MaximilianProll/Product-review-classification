{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Bad Product Reviews: Deep Learning Model\n",
    "\n",
    "**Team J Cubed**: James Wei, Jessica Ko, Jay Patel\n",
    "\n",
    "CS 294-129, Fall 2016\n",
    "\n",
    "**NOTE:** Most of the code for data handling, featurization, and model construction lies outside of this notebook. Please see the Python scripts in {src_root}/lib for exact implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from lib.amazon_model import *\n",
    "from lib.data_mgmt import *\n",
    "from nltk.data import find\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset/Featurization Config\n",
    "path = 'data/full_cols_bad_50k.csv'\n",
    "label = 'bad'\n",
    "max_size = 200\n",
    "max_tfidf = 12000\n",
    "n_classes = 2\n",
    "n_features = 300\n",
    "test_size = 0.25\n",
    "verbose = True\n",
    "\n",
    "# LSTM Config\n",
    "lstm_size = 50\n",
    "num_lstm_layers = 2\n",
    "lstm_dropout = 0.15\n",
    "\n",
    "# FC Config\n",
    "fc_layer_sizes = [75, 50, 30]\n",
    "fc_dropout = 0.10\n",
    "\n",
    "# Training config\n",
    "run_name = label + '_run_' + path.split('.')[0].split('_')[-1]\n",
    "learning_rate = 1e-3\n",
    "reg_weight = 1e-5\n",
    "training_iters = 15000 * 10\n",
    "batch_size = 15\n",
    "display_step = 10\n",
    "save_every = 5000\n",
    "save_fn = 'chkpts/model_' + run_name + '.ckpt'\n",
    "should_log_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_loss_iter_fn = 'log/training_loss_iter_' + run_name + '.csv'\n",
    "training_acc_iter_fn = 'log/training_acc_iter_' + run_name + '.csv'\n",
    "testing_loss_iter_fn = 'log/testing_loss_iter_' + run_name + '.csv'\n",
    "testing_acc_iter_fn = 'log/testing_acc_iter_' + run_name + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.97 s, sys: 56 ms, total: 8.02 s\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "we_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset at data/full_cols_bad_50k.csv.\n",
      "Splitting training and test sets.\n",
      "Building reviewer/ASIN maps.\n",
      "Fitting tf-idf featurizer.\n",
      "doc list created: 50000\n",
      "Tuning tf-idf classifier.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/44/prollm1/unix/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = Dataset(path, we_model, {\n",
    "    'label'      : label,\n",
    "    'max_size'   : max_size,\n",
    "    'max_tfidf'  : max_tfidf,\n",
    "    'n_features' : n_features,\n",
    "    'test_size'  : test_size,\n",
    "    'verbose'    : verbose\n",
    "})\n",
    "\n",
    "n_cols = dataset.get_n_cols() # Number of additional DNN features (including tf-idf features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelConfig(object):\n",
    "    def __init__(self):\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm_keep_prob = 1.0 - lstm_dropout\n",
    "        self.fc_keep_prob = 1.0 - fc_dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_weight = reg_weight\n",
    "        self.fc_layer_sizes = fc_layer_sizes\n",
    "        self.n_classes = n_classes\n",
    "        self.n_cols = n_cols\n",
    "        self.n_fc_layers = len(fc_layer_sizes) + 1\n",
    "\n",
    "config = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "amazon_model = AmazonModel(config)\n",
    "pred = amazon_model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input placeholders\n",
    "x = tf.placeholder(\"float\", [None, max_size, n_features])\n",
    "x2 = tf.placeholder(\"float\", [None, n_cols])\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "lstm_keep_prob = tf.placeholder(tf.float32)\n",
    "fc_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Output placeholder\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Checkpointing\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run forward pass\n",
    "logits, weights = pred(x, x2, seqlen, lstm_keep_prob, fc_keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.add(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)),\n",
    "              amazon_model.regularization_penalty())\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if should_log_metrics:\n",
    "    train_loss_log = open(training_loss_iter_fn, 'a')\n",
    "    test_loss_log = open(testing_loss_iter_fn, 'a')\n",
    "    train_acc_log = open(training_acc_iter_fn, 'a')\n",
    "    test_acc_log = open(testing_acc_iter_fn, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_test_acc = -1.0\n",
    "sess = tf.Session()\n",
    "\n",
    "# Launch the graph\n",
    "if True:\n",
    "    sess.run(init)\n",
    "    test_data, test_label, test_seqlen, test_dnn = dataset.get_test_batch()\n",
    "    test_dict = {\n",
    "        x: test_data, \n",
    "        y: test_label,\n",
    "        seqlen: test_seqlen, \n",
    "        x2: test_dnn,\n",
    "        lstm_keep_prob: 1.00,\n",
    "        fc_keep_prob: 1.00\n",
    "    }\n",
    "    step = 1\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y, batch_seqlen, batch_dnn = dataset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={\n",
    "            x: batch_x, \n",
    "            y: batch_y,\n",
    "            seqlen: batch_seqlen, \n",
    "            x2: batch_dnn,\n",
    "            lstm_keep_prob: config.lstm_keep_prob,\n",
    "            fc_keep_prob: config.fc_keep_prob\n",
    "        })\n",
    "        \n",
    "        eval_dict = {\n",
    "            x:      batch_x, \n",
    "            y:      batch_y,\n",
    "            seqlen: batch_seqlen, \n",
    "            x2:     batch_dnn,\n",
    "            lstm_keep_prob: 1.00,\n",
    "            fc_keep_prob:   1.00\n",
    "        }\n",
    "        \n",
    "        if should_log_metrics:\n",
    "            acc = sess.run(accuracy, feed_dict=eval_dict)\n",
    "            loss = sess.run(cost, feed_dict=eval_dict)\n",
    "            line_start = str(step*batch_size) + \",\"\n",
    "            train_loss_log.write(line_start + str(loss) + '\\n')\n",
    "            train_loss_log.flush()\n",
    "            train_acc_log.write(line_start + str(acc) + '\\n')\n",
    "            train_acc_log.flush()\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            if not should_log_metrics:\n",
    "                acc = sess.run(accuracy, feed_dict=eval_dict)\n",
    "                loss = sess.run(cost, feed_dict=eval_dict)\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "            \n",
    "        if step % (display_step) == 0:\n",
    "            if should_log_metrics:\n",
    "                test_acc = sess.run(accuracy, feed_dict=test_dict)\n",
    "                test_loss = sess.run(cost, feed_dict=test_dict)\n",
    "                test_loss_log.write(line_start + str(test_loss) + '\\n')\n",
    "                test_loss_log.flush()\n",
    "                test_acc_log.write(line_start + str(test_acc) + '\\n')\n",
    "                test_acc_log.flush()        \n",
    "            if step % (5 * display_step) == 0:\n",
    "                if not should_log_metrics:\n",
    "                    test_acc = sess.run(accuracy, feed_dict=test_dict)\n",
    "                    test_loss = sess.run(cost, feed_dict=test_dict)\n",
    "                print(\"Validation Accuracy:\", test_acc)\n",
    "                print(\"Validation Loss:\", test_loss)\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    best_test_acc = test_acc\n",
    "                    # Plot ROC\n",
    "                    answers = sess.run(tf.nn.softmax(logits), feed_dict=test_dict)\n",
    "                    if label == 'good':\n",
    "                        fpr, tpr, _ = roc_curve(1 - np.argmax(test_label, axis=1), answers[:, 0])\n",
    "                    else:\n",
    "                        fpr, tpr, _ = roc_curve(np.argmax(test_label, axis=1), answers[:, 1])\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    plt.figure()\n",
    "                    lw = 2\n",
    "                    plt.plot(fpr, tpr, color='darkorange',\n",
    "                             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "                    plt.xlim([0.0, 1.0])\n",
    "                    plt.ylim([0.0, 1.05])\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    plt.ylabel('True Positive Rate')\n",
    "                    plt.title('ROC Curve')\n",
    "                    plt.legend(loc=\"lower right\")\n",
    "                    plt.savefig('plots/' + run_name + '_step_' + str(step) + '.png', dpi=300, bbox_inches='tight')\n",
    "                    plt.show()\n",
    "        \n",
    "        if step % save_every == 0:\n",
    "            save_path = saver.save(sess, save_fn)\n",
    "            print(\"Checkpoint saved at %s\" % save_path)\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_fn)\n",
    "    print(\"Checkpoint saved at %s\" % save_path)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(\"Final Testing Accuracy:\", sess.run(accuracy, feed_dict=test_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read out the final weight matrix and save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = sess.run(weights)\n",
    "n_fc = len(fc_layer_sizes) + 1\n",
    "\n",
    "for i in range(n_fc):\n",
    "    f_name = 'weights/' + label + '_dim_' + str(i) + '.csv'\n",
    "    np.savetxt(f_name, w[i], delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
