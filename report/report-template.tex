\documentclass[a4paper,11pt]{article}

\usepackage[english]{babel} 			%% englische Sprache

\usepackage[latin1,applemac]{inputenc}	%% deutsche Umlaute wie normale
 								%% Buchstaben verwenden 
 								%% (ansonsten muesste ‰ durch a getippt werden)
\usepackage{a4wide} 				%% kleinere Seitenränder

\usepackage{amssymb,amsthm,amsfonts, amsmath}
								%% diverse Matheerweiterungen, z.B. \implies
 								%% diverse Matheerweiterungen, z.B. \mathbb{R}
%\usepackage{stmaryrd} 				%% weitere Symbole
\usepackage{epsfig} 					%% um eps-Dateien einzubinden (\epsfig{file=...})
\usepackage{longtable} 				%% fuer Tabellen ueber mehrere Seiten
\usepackage{color}
\usepackage{hyperref}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{multirow}
\usepackage{float}

\hypersetup{						%get rid of red box around hyperlink
pdfborder = {0 0 0}
}

\usepackage{listings} 				% noice code inclusion
\usepackage{color}

\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\lstset{
	frame=single,
	language=Python,
	belowcaptionskip=1\baselineskip,
	breaklines=true,
	frame=tb,
	showstringspaces=false,
	basicstyle=\footnotesize\ttfamily,
	keywordstyle=\color{deepblue},
	emphstyle=\color{deepred},    		% Custom highlighting style
	stringstyle=\color{deepgreen},
	commentstyle=\itshape\color{deepgreen}
}


\begin{document}

\title{Title of the Mini Project\\
\normalsize (MP 004 Firing Neurons)}

\author{Héctor Laria Mantecón (662134) \and Maximilian Proll (662529)
  \and Aditya Kaushik Surikuchi (662862)}

\maketitle

\newcommand{\points}[1]{\par\noindent\textit{(#1 points)}}
\newcommand{\onepoint}{\par\noindent\textit{(1 point)}}
\newcommand{\defaulttext}[1]{\textit{\textcolor{red}{#1}}}

\begin{abstract}
  \defaulttext{Write an abstract of approximately 200 words. \\
  The overall length of your report in this format should be 6--8
  pages, including images, references and everything, in the format of
  this template.  You can choose to use any document template and
  typesetting facility as long as you submit your report as a PDF file.
  Use of figures and tables is strongly encouraged!\\
  The breakdown of the total 30 points of the mini project is shown
  below for all sections of the report. However, points will be given
  only for project reports that are complete, i.e.\@ they have
  relevant content in all sections.  Minimum of 10 points are required
  for passing the course.  \\
  The reports will be evaluated using the Turnitin plagiarism
  prevention tool to ensure that the reports are genuine work written
  for this course.}
  %% 
  \onepoint
\end{abstract}

\section{Introduction}

\defaulttext{Describe the machine learning problem you have addressed in your mini
project, your applied method and used data in general terms.}
%%
\points{2}

\section{Related work}

There are many proposed solutions for text classification problems. They involve both neural and non-neural techniques. Some of the prior work on Amazon movie reviews has shown that ensemble learning using derived features (like review length, product rating) which performed better in comparison to plain classification methods like SVMs or logistic regression.
Other work used contextual metadata features (like product category, reviewer details) which seemed to work when using non-neural models.

Around 2011, deep networks starting becoming popular after the conception of convolutional neural nets\cite{DBLP:journals/corr/Kim14f} for sentence classification. They started to consistently perform better compared to the conventional classification methods listed above. Gradually when RNNs (recurrent neural nets) came into picture researchers started to take advantage of the architecture. Specifically, the introduction of LSTMs started to revolutionize the challenging domains of speech recognition, machine translation and image captioning.

Owing to such growing popularity of LSTMs there were many deep network architectures researched, built and published leveraging multiple-layers of LSTMs for the natural language processing domain. One such notable work is for predicting product review helpfulness using a deep net arch with 2 LSTM layers along with sophisticated NLP features\cite{wei}. 
%%

\section{Method}

\defaulttext{Describe your chosen solution in sufficient detail.  Pay special
attention to describing the deep learning algorithm(s) you used.  Show
some essential equations and describe all hyper-parameters that are
involved in the method.}
%%
\points{3}

\section{Data}

For the \cite{HeMcA16a} and \cite{McATarShiHen15}
\defaulttext{Tell where the data is from, what it contains, the number of samples
in training, validation and test sets, the dimensional, etc.  Also
describe the preprocessing stages applied by the providers of the data
or by you yourselves.}
%%
\points{3}

\section{Experiments}

\defaulttext{Explain the goal and implementation of your experiments.  Tell what
hyperparameters values you experimented with and what other
variations in the method you tested.}
%%
\points{5}

As stated in our methodology, we started evaluating baseline models to prove their assertions true. We found a very convincing architecture which performed very well on the test set, so we proceeded to implement in \textit{TensorFlow}\defaulttext{cite here}. This library is of relatively easy use and has many modules already implemented. We also used \textit{nltk} for word stemming, \textit{gensim} for word embedding, \textit{pandas} to handle the datasets, \textit{sk-learn} for t-SNE computation and \textit{matplotlib} for visualizations.

The schema implemented is explained next. The design variations are discussed following to it.

\subsection{Main model framework}
The approach, as seen in Figure\defaulttext{cite here}, is actually very intuitive. At the end we have one set of fully-connected layers with a softmax function. They are configured with length $75, 50$ and $30$, leakyRELU ($max(0.01x, x)$) as the activation function and dropout as regularization This network is fed with a number of high-level input features. Namely:\defaulttext{put the hyperparameters too}
\begin{description}
	\item[Product id, reviewer id, category, rating] We wanted to take into account the crucial relations of the product reviews with their source; the reviewer themself. I.e. some people explain thing more concisely than others. For that purpose we feed the network with those labels, taken directly from the dataset.
	
	The category also makes sense to appear, as some people have more expertise than other on certain areas. Which allows them to produce higher-quality reviews.
	
	The rating is also evident, as it's the numerical opinion of the product by the reviewer.
	
	\item[Anatomical encoder] We used an encoder to extract common Natural Language Processing features we found very valuables in the baseline model, like the number of tokens, the length of those, the number of characters in the token, etc. Which are produced by using techniques like:
	\begin{enumerate}
		\item tf-idf featurization
		\item stemming
		\item unigram normalization
		\item word2vec
	\end{enumerate}
	
	\item[Logistic regression] We applied logistic regression with $l_2$ regularization to the td-idf features of the top 12.000 text reviews words, stemmed to reduce the set of $n$-grams to consider. We have two regression models to classify good and bad reviews respectively. There is only one hyper-parameter, a regularization variable to avoid overfitting. With simple binary search we could find the optimal value at $1$ for the good reviews dataset and $0.23$ for the bad reviews one.
	
	\item[Recurrent networks] dropout adam. were fed with a word2vec model of the 
\end{description}

\defaulttext{barplot here}

we tried with some variations

\subsection{LSTMs variations}
cell size

\subsection{GRUs}
as explained in the methods
Adam

\subsection{NLP features relevance}
we took them out

\section{Results}

\defaulttext{Give the results of your experiments in a table and explain them in
	the text.  Some figures would be good here for illustration.  Refer to
	the table(s) and image(s) in the text and describe them.}
%%
\points{5}

table: time  performance
lstms
grus
nlp features

\subsection{LSTM vs GRU}

\section{Discussion}

\defaulttext{Discuss your results in comparison with comparable results your have
found in the literature and web.  Explain any other findings you made
while running the experiments.}
%%
\points{4}

\section{Conclusions}

\defaulttext{Give your final conclusions from the whole mini project and its results.}
%%
\points{3}

\section{References}

\bibliography{bibliography}{}
\bibliographystyle{acm}

\defaulttext{Most likely between 5--10 references to related works and results.}
%%
\points{2}

\section{Roles of the authors}

\defaulttext{If you have more than one member in your mini project group, you need
to explain in this section how the labor was divided between you and
what were each one's roles in the project and its reporting.}

\end{document}