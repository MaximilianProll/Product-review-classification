{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Good Product Reviews: Deep Learning Model\n",
    "\n",
    "**Team J Cubed**: James Wei, Jessica Ko, Jay Patel\n",
    "\n",
    "CS 294-129, Fall 2016\n",
    "\n",
    "**NOTE:** Most of the code for data handling, featurization, and model construction lies outside of this notebook. Please see the Python scripts in {src_root}/lib for exact implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/44/prollm1/unix/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/u/44/prollm1/unix/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from gensim.models import Word2Vec\n",
    "from lib.amazon_model import *\n",
    "from lib.data_mgmt import *\n",
    "from nltk.data import find\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset/Featurization Config\n",
    "path = 'data/full_cols_good.csv'\n",
    "label = 'good'\n",
    "max_size = 200\n",
    "max_tfidf = 12000\n",
    "n_classes = 2\n",
    "n_features = 300\n",
    "test_size = 0.25\n",
    "verbose = True\n",
    "\n",
    "# LSTM Config\n",
    "lstm_size = 50\n",
    "num_lstm_layers = 2\n",
    "lstm_dropout = 0.15\n",
    "\n",
    "# FC Config\n",
    "fc_layer_sizes = [75, 50, 30]\n",
    "fc_dropout = 0.10\n",
    "\n",
    "# Training config\n",
    "run_name = 'genesisgood_run1'\n",
    "learning_rate = 1e-3\n",
    "reg_weight = 1e-5\n",
    "training_iters = 15000 * 10\n",
    "batch_size = 15\n",
    "display_step = 10\n",
    "save_every = 5000\n",
    "save_fn = 'chkpts/model_' + run_name + '.ckpt'\n",
    "should_log_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_loss_iter_fn = 'log/training_loss_iter_' + run_name + '.csv'\n",
    "training_acc_iter_fn = 'log/training_acc_iter_' + run_name + '.csv'\n",
    "testing_loss_iter_fn = 'log/testing_loss_iter_' + run_name + '.csv'\n",
    "testing_acc_iter_fn = 'log/testing_acc_iter_' + run_name + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "we_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset at data/full_cols_good.csv.\n",
      "Splitting training and test sets.\n",
      "Building reviewer/ASIN maps.\n",
      "Fitting tf-idf featurizer.\n",
      "doc list created: 200000\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-759ccf6a95ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'n_features'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m'test_size'\u001b[0m  \u001b[0;34m:\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m'verbose'\u001b[0m    \u001b[0;34m:\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m })\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/home/home4/44/prollm1/unix/DL-project/lib/data_mgmt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, we_model, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfFeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'review_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/m/home/home4/44/prollm1/unix/DL-project/lib/data_mgmt.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         )\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset(path, we_model, {\n",
    "    'label'      : label,\n",
    "    'max_size'   : max_size,\n",
    "    'max_tfidf'  : max_tfidf,\n",
    "    'n_features' : n_features,\n",
    "    'test_size'  : test_size,\n",
    "    'verbose'    : verbose\n",
    "})\n",
    "\n",
    "n_cols = dataset.get_n_cols() # Number of additional DNN features (including tf-idf features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelConfig(object):\n",
    "    def __init__(self):\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "        self.lstm_keep_prob = 1.0 - lstm_dropout\n",
    "        self.fc_keep_prob = 1.0 - fc_dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_weight = reg_weight\n",
    "        self.fc_layer_sizes = fc_layer_sizes\n",
    "        self.n_classes = n_classes\n",
    "        self.n_cols = n_cols\n",
    "        self.n_fc_layers = len(fc_layer_sizes) + 1\n",
    "\n",
    "config = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "amazon_model = AmazonModel(config)\n",
    "pred = amazon_model.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input placeholders\n",
    "x = tf.placeholder(\"float\", [None, max_size, n_features])\n",
    "x2 = tf.placeholder(\"float\", [None, n_cols])\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "lstm_keep_prob = tf.placeholder(tf.float32)\n",
    "fc_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Output placeholder\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Checkpointing\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run forward pass\n",
    "logits = pred(x, x2, seqlen, lstm_keep_prob, fc_keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.add(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)),\n",
    "              amazon_model.regularization_penalty())\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if should_log_metrics:\n",
    "    train_loss_log = open(training_loss_iter_fn, 'a')\n",
    "    test_loss_log = open(testing_loss_iter_fn, 'a')\n",
    "    train_acc_log = open(training_acc_iter_fn, 'a')\n",
    "    test_acc_log = open(testing_acc_iter_fn, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_test_acc = -1.0\n",
    "sess = tf.Session()\n",
    "\n",
    "# Launch the graph\n",
    "if True:\n",
    "    sess.run(init)\n",
    "    test_data, test_label, test_seqlen, test_dnn = dataset.get_test_batch()\n",
    "    test_dict = {\n",
    "        x: test_data, \n",
    "        y: test_label,\n",
    "        seqlen: test_seqlen, \n",
    "        x2: test_dnn,\n",
    "        lstm_keep_prob: 1.00,\n",
    "        fc_keep_prob: 1.00\n",
    "    }\n",
    "    step = 1\n",
    "    \n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y, batch_seqlen, batch_dnn = dataset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={\n",
    "            x: batch_x, \n",
    "            y: batch_y,\n",
    "            seqlen: batch_seqlen, \n",
    "            x2: batch_dnn,\n",
    "            lstm_keep_prob: config.lstm_keep_prob,\n",
    "            fc_keep_prob: config.fc_keep_prob\n",
    "        })\n",
    "        \n",
    "        eval_dict = {\n",
    "            x:      batch_x, \n",
    "            y:      batch_y,\n",
    "            seqlen: batch_seqlen, \n",
    "            x2:     batch_dnn,\n",
    "            lstm_keep_prob: 1.00,\n",
    "            fc_keep_prob:   1.00\n",
    "        }\n",
    "        \n",
    "        if should_log_metrics:\n",
    "            acc = sess.run(accuracy, feed_dict=eval_dict)\n",
    "            loss = sess.run(cost, feed_dict=eval_dict)\n",
    "            line_start = str(step*batch_size) + \",\"\n",
    "            train_loss_log.write(line_start + str(loss) + '\\n')\n",
    "            train_loss_log.flush()\n",
    "            train_acc_log.write(line_start + str(acc) + '\\n')\n",
    "            train_acc_log.flush()\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            if not should_log_metrics:\n",
    "                acc = sess.run(accuracy, feed_dict=eval_dict)\n",
    "                loss = sess.run(cost, feed_dict=eval_dict)\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "            \n",
    "        if step % (display_step) == 0:\n",
    "            if should_log_metrics:\n",
    "                test_acc = sess.run(accuracy, feed_dict=test_dict)\n",
    "                test_loss = sess.run(cost, feed_dict=test_dict)\n",
    "                test_loss_log.write(line_start + str(test_loss) + '\\n')\n",
    "                test_loss_log.flush()\n",
    "                test_acc_log.write(line_start + str(test_acc) + '\\n')\n",
    "                test_acc_log.flush()        \n",
    "            if step % (5 * display_step) == 0:\n",
    "                if not should_log_metrics:\n",
    "                    test_acc = sess.run(accuracy, feed_dict=test_dict)\n",
    "                    test_loss = sess.run(cost, feed_dict=test_dict)\n",
    "                print(\"Validation Accuracy:\", test_acc)\n",
    "                print(\"Validation Loss:\", test_loss)\n",
    "                \n",
    "                if test_acc > best_test_acc:\n",
    "                    best_test_acc = test_acc\n",
    "                    # Plot ROC\n",
    "                    answers = sess.run(tf.nn.softmax(logits), feed_dict=test_dict)\n",
    "                    if label == 'good':\n",
    "                        fpr, tpr, _ = roc_curve(1 - np.argmax(test_label, axis=1), answers[:, 0])\n",
    "                    else:\n",
    "                        fpr, tpr, _ = roc_curve(np.argmax(test_label, axis=1), answers[:, 1])\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    plt.figure()\n",
    "                    lw = 2\n",
    "                    plt.plot(fpr, tpr, color='darkorange',\n",
    "                             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "                    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "                    plt.xlim([0.0, 1.0])\n",
    "                    plt.ylim([0.0, 1.05])\n",
    "                    plt.xlabel('False Positive Rate')\n",
    "                    plt.ylabel('True Positive Rate')\n",
    "                    plt.title('ROC Curve')\n",
    "                    plt.legend(loc=\"lower right\")\n",
    "                    plt.show()\n",
    "        \n",
    "        if step % save_every == 0:\n",
    "            save_path = saver.save(sess, save_fn)\n",
    "            print(\"Checkpoint saved at %s\" % save_path)\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    save_path = saver.save(sess, save_fn)\n",
    "    print(\"Checkpoint saved at %s\" % save_path)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    print(\"Final Testing Accuracy:\", sess.run(accuracy, feed_dict=test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
